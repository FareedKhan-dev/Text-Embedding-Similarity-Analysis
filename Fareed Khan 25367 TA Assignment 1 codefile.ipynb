{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Name: Fareed Hassan Khan\n",
    "## ERP ID: 25367\n",
    "## Text Analytics Assignment 1"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"border: 1px solid black; padding:10px; border-radius: 5px;\">\n",
    "<p>Important Point</p>\n",
    "\n",
    "Multiple functions has been created to avoid repetitive code.\n",
    "    \n",
    "| Function Name | Purpose |\n",
    "| :--- | :--- |\n",
    "| lemma_or_stemma | To lemmatized and stemmed news titles | \n",
    "| cleaning | To remove unwanted characters using regex | \n",
    "| sentence_vector | To transform using pre-trained word2vec or glove or customized word2vec dataset into vectorized matrix | \n",
    "| assign_doc_to_clus | map each document to a cluster based on the method we have used | \n",
    "</div>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset Description\n",
    "\n",
    "I explored the BBC News dataset on Kaggle, which contains over 14,000 news articles published by the British Broadcasting Corporation (BBC) over a six-year period. The dataset covers five categories, namely business, entertainment, politics, sport, and technology. Moreover, each news article is accompanied by a short description, giving a quick insight into its contents.\n",
    "\n",
    "| Dataset Name | Default task | Download link |\n",
    "| :--- | :--- | :--- |\n",
    "| BBC News | Clustering, Classification | https://www.kaggle.com/datasets/gpreda/bbc-news |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_____"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Project Workflow**\n",
    "1. [Initial steps](#initial)\n",
    "    1. Importing Libraries\n",
    "    2. Creating Function\n",
    "2. [Models](#ml-method)\n",
    "3. [Clustering KMeans](#kmeans)\n",
    "4. [Saving Models](#savingmodels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_________"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"initial\"></a>\n",
    "# Initial Steps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To remove warning\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# For dataset handling\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# For working with models\n",
    "import nltk\n",
    "import re\n",
    "import itertools\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "from gensim.models import KeyedVectors, Word2Vec\n",
    "import gensim \n",
    "import logging\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n",
    "\n",
    "# Checking similar documents\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.cluster import KMeans, AgglomerativeClustering, MiniBatchKMeans, MeanShift\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "\n",
    "# For saving models\n",
    "import pickle"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BBC News Dataset loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('bbc_news.csv', usecols=['title','description'])\n",
    "df.drop_duplicates(inplace=True)\n",
    "df.dropna(inplace=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cleaning Unwanted Characters (Regex) function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleaning(s):\n",
    "    s = str(s)\n",
    "    s = re.sub('[!@#$_]', '', s)\n",
    "    s = s.replace(\"co\",\"\")\n",
    "    s = s.replace(\"https\",\"\")\n",
    "    s = s.replace(\"[\\w*\",\" \")\n",
    "    s = s.replace('<.*?>', '')\n",
    "    s = s.replace('strong>', '')\n",
    "    s = s.replace('\\x92', '')\n",
    "    return s\n",
    "\n",
    "df['title'] = df['title'].apply(cleaning)\n",
    "df['description'] = df['description'].apply(cleaning)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Applying Lemmatization/Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>description</th>\n",
       "      <th>headline_lemmatization</th>\n",
       "      <th>headline_stemming</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Ukraine: Angry Zelensky vows to punish Russian...</td>\n",
       "      <td>The Ukrainian president says the untry will no...</td>\n",
       "      <td>ukraine: angry zelensky vow to punish russian ...</td>\n",
       "      <td>ukraine: angri zelenski vow to punish russian ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>War in Ukraine: Taking ver in a town under attack</td>\n",
       "      <td>Jeremy Bowen was on the frontline in Irpin, as...</td>\n",
       "      <td>war in ukraine: taking ver in a town under attack</td>\n",
       "      <td>war in ukraine: take ver in a town under attack</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title  \\\n",
       "0  Ukraine: Angry Zelensky vows to punish Russian...   \n",
       "1  War in Ukraine: Taking ver in a town under attack   \n",
       "\n",
       "                                         description  \\\n",
       "0  The Ukrainian president says the untry will no...   \n",
       "1  Jeremy Bowen was on the frontline in Irpin, as...   \n",
       "\n",
       "                              headline_lemmatization  \\\n",
       "0  ukraine: angry zelensky vow to punish russian ...   \n",
       "1  war in ukraine: taking ver in a town under attack   \n",
       "\n",
       "                                   headline_stemming  \n",
       "0  ukraine: angri zelenski vow to punish russian ...  \n",
       "1    war in ukraine: take ver in a town under attack  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ps = PorterStemmer()\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def lemma_or_stemma(text, which_method):\n",
    "\n",
    "    if which_method == 'stemming':\n",
    "        stem_list = []\n",
    "        for i in text.split():\n",
    "            stem_list.append(ps.stem(i.lower()))\n",
    "        return \" \".join(stem_list)\n",
    "    else:\n",
    "        lem_list = []\n",
    "        for i in text.split():\n",
    "            lem_list.append(lemmatizer.lemmatize(i.lower()))\n",
    "        return \" \".join(lem_list)\n",
    "\n",
    "df['headline_lemmatization'] = df['title'].apply(lambda text: lemma_or_stemma(text, 'lemmatization'))\n",
    "df['headline_stemming'] = df['title'].apply(lambda text: lemma_or_stemma(text, 'stemming'))\n",
    "\n",
    "df.head(2)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "sentence_vector (transforming documents to vectorized matrix using word2vec, glove etc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentence_vector(sentence, word_vectors):\n",
    "    words = sentence.lower().split()\n",
    "    vectors = []\n",
    "    for word in words:\n",
    "        if word in word_vectors:\n",
    "            vectors.append(word_vectors[word])\n",
    "        if not vectors:\n",
    "            vectors.append(np.zeros(300))\n",
    "    return np.mean(vectors, axis=0)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"Models\"></a>\n",
    "# Models"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bag of words default (CountVectorizer/tf-idf Vectorizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CountVectorizer vector Shape (14039, 79576) ________ Tf-Idf Vector Shape (14039, 79576)\n"
     ]
    }
   ],
   "source": [
    "# CountVectorizer\n",
    "cv = CountVectorizer(ngram_range=(1,2), stop_words='english')\n",
    "# TfIdf-Vectorizer\n",
    "tfidf = TfidfVectorizer(ngram_range=(1,2), stop_words='english')\n",
    "\n",
    "# Applying default bag of words on lemmatize documents\n",
    "lemma_vector_countvect = cv.fit_transform(df['headline_lemmatization']).toarray()\n",
    "lemma_vector_tfidf = tfidf.fit_transform(df['headline_lemmatization']).toarray()\n",
    "\n",
    "print(f'CountVectorizer vector Shape {lemma_vector_countvect.shape} ________ Tf-Idf Vector Shape {lemma_vector_tfidf.shape}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bag of Words"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parameter tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CountVectorizer Parameters\n",
    "parameters_countvectorizer = {\n",
    "    'n_gram': [(1,2)],\n",
    "    'max_features': [10000, 20000, 25000],\n",
    "    'binary':[True, False]}\n",
    "keys, values = zip(*parameters_countvectorizer.items())\n",
    "parameters_countvectorizer = [dict(zip(keys, v)) for v in itertools.product(*values)]\n",
    "\n",
    "\n",
    "# TF-IDF Parameters\n",
    "parameters_tfidfvectorizer = {\n",
    "    'n_gram': [(1,2)],\n",
    "    'max_features': [30000, 50000, 60000],\n",
    "    'norm':['l1','l2']}\n",
    "keys, values = zip(*parameters_tfidfvectorizer.items())\n",
    "parameters_tfidfvectorizer = [dict(zip(keys, v)) for v in itertools.product(*values)]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CountVectorizer (Stemming/Lemmatization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "stemming_data_countvec = []\n",
    "lemmatization_data_countvect = []\n",
    "\n",
    "for each_model in parameters_countvectorizer:\n",
    "    # Count Vectorizer with stemming/lemmatization\n",
    "    cv = CountVectorizer(ngram_range=each_model['n_gram'], stop_words='english', \n",
    "                         max_features=each_model['max_features'], binary=each_model['binary'])\n",
    "    lemma_vector_countvect = cv.fit_transform(df['headline_lemmatization']).toarray()\n",
    "    stemm_vector_countvect = cv.fit_transform(df['headline_stemming']).toarray()\n",
    "\n",
    "    lemmatization_data_countvect.append(lemma_vector_countvect)\n",
    "    stemming_data_countvec.append(stemm_vector_countvect)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "each_similaririty_lemma_countvect = []\n",
    "each_similaririty_stem_countvect = []\n",
    "\n",
    "for each in lemmatization_data_countvect:\n",
    "    similarity_scores = cosine_similarity(each)\n",
    "    each_similaririty_lemma_countvect.append(similarity_scores)\n",
    "\n",
    "for each in stemming_data_countvec:\n",
    "    similarity_scores = cosine_similarity(each)\n",
    "    each_similaririty_stem_countvect.append(similarity_scores)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tf-Idf Vectorizer (Stemming/Lemmatization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "stemming_data_tidf = []\n",
    "lemmatization_data_tidf = []\n",
    "\n",
    "for each_model in parameters_tfidfvectorizer:\n",
    "    # Count Vectorizer with stemming/lemmatization\n",
    "    cv = TfidfVectorizer(ngram_range=each_model['n_gram'], stop_words='english', max_features=each_model['max_features'], \n",
    "                         norm=each_model['norm'])\n",
    "    lemmatization_vector_tidf = cv.fit_transform(df['headline_lemmatization']).toarray()\n",
    "    stemming_vector_tidf = cv.fit_transform(df['headline_stemming']).toarray()\n",
    "\n",
    "    lemmatization_data_tidf.append(lemmatization_vector_tidf)\n",
    "    stemming_data_tidf.append(stemming_vector_tidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "each_similaririty_lemma_tfidf = []\n",
    "each_similaririty_stem_tfidf = []\n",
    "\n",
    "for each in lemmatization_data_tidf:\n",
    "    similarity_scores = cosine_similarity(each)\n",
    "    each_similaririty_lemma_tfidf.append(similarity_scores)\n",
    "\n",
    "for each in stemming_data_tidf:\n",
    "    similarity_scores = cosine_similarity(each)\n",
    "    each_similaririty_stem_tfidf.append(similarity_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Countvectorizer lemmatization/stemming cosine similarity with hyper-parameter tuning\n",
    "each_similaririty_lemma_countvect\n",
    "each_similaririty_stem_countvect\n",
    "\n",
    "# Tf-Idf lemmatization/stemming cosine similarity with hyper-parameter tuning\n",
    "each_similaririty_lemma_tfidf\n",
    "each_similaririty_stem_tfidf"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word2Vec Pre-trained"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-12 14:13:52,007 : INFO : loading projection weights from GoogleNews-vectors-negative300.bin\n",
      "2023-03-12 14:13:52,351 : INFO : KeyedVectors lifecycle event {'msg': 'loaded (50000, 300) matrix of type float32 from GoogleNews-vectors-negative300.bin', 'binary': True, 'encoding': 'utf8', 'datetime': '2023-03-12T14:13:52.351055', 'gensim': '4.3.0', 'python': '3.9.0 (tags/v3.9.0:9cf6752, Oct  5 2020, 15:34:40) [MSC v.1927 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.22621-SP0', 'event': 'load_word2vec_format'}\n"
     ]
    }
   ],
   "source": [
    "model_W2V = KeyedVectors.load_word2vec_format('GoogleNews-vectors-negative300.bin',binary=True, limit=50000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2vec_news = df['title'].apply(lambda sentence: sentence_vector(sentence, model_W2V))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe = list(word2vec_news)\n",
    "cosine_similarity_word2vec = cosine_similarity(dataframe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 378,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "actual news ---- \n",
      " England v New Zealand: Late wickets put hosts in charge at Headingley\n",
      "similar news -----\n",
      "England v New Zealand: Late wickets put England in charge of third Test\n",
      "England v South Africa: Issy Wong takes late wickets to boost hosts\n",
      "England v India highlights: Jasprit Bumrah takes six wickets as hosts beaten by 10 wickets in first ODI\n"
     ]
    }
   ],
   "source": [
    "news_id = 4511\n",
    "\n",
    "print('actual news ----', '\\n', df.iloc[news_id].title)\n",
    "the_sort = sorted(list(enumerate(cosine_similarity_word2vec[news_id])), reverse=True, key=lambda x:x[1])[0:10]\n",
    "\n",
    "print('similar news -----')\n",
    "# close_ques = []\n",
    "for each in the_sort[1:4]:\n",
    "    # close_ques.append(each[0])\n",
    "    print(df.iloc[each[0]].title)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Glove Pre-trained"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-12 12:39:13,908 : INFO : loading projection weights from glove.6B.50d.txt.word2vec\n",
      "2023-03-12 12:39:24,851 : INFO : KeyedVectors lifecycle event {'msg': 'loaded (400000, 50) matrix of type float32 from glove.6B.50d.txt.word2vec', 'binary': False, 'encoding': 'utf8', 'datetime': '2023-03-12T12:39:24.851510', 'gensim': '4.3.0', 'python': '3.9.0 (tags/v3.9.0:9cf6752, Oct  5 2020, 15:34:40) [MSC v.1927 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.22621-SP0', 'event': 'load_word2vec_format'}\n"
     ]
    }
   ],
   "source": [
    "from gensim.scripts.glove2word2vec import glove2word2vec\n",
    "\n",
    "# glove_input_file = 'glove.6B.50d.txt'\n",
    "word2vec_output_file = 'glove.6B.300d.txt.word2vec'\n",
    "# glove2word2vec(glove_input_file,word2vec_output_file)\n",
    "modelg = KeyedVectors.load_word2vec_format(word2vec_output_file,binary=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "glove_news = df['title'].apply(lambda sentence: sentence_vector(sentence, modelg))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 383,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe_glove = list(glove_news)\n",
    "cosine_similarity_glove = cosine_similarity(dataframe_glove)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 390,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "actual news ---- \n",
      " Track Cycling World Championships: Jess Roberts wins women's scratch bronze\n",
      "similar news -----\n",
      "Commonwealth Games: Laura Kenny wins scratch race gold at track cycling\n",
      "Track Cycling World Championships: Great Britain win three bronze medals on opening day\n",
      "World Aquatic Championships: Summer McIntosh, 15, wins gold & Great Britain take relay bronze\n"
     ]
    }
   ],
   "source": [
    "news_id = 8777\n",
    "\n",
    "print('actual news ----', '\\n', df.iloc[news_id].title)\n",
    "the_sort = sorted(list(enumerate(qw[news_id])), reverse=True, key=lambda x:x[1])[0:10]\n",
    "\n",
    "print('similar news -----')\n",
    "# close_ques = []\n",
    "for each in the_sort[1:4]:\n",
    "    print(df.iloc[each[0]].title)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Customized Word2Vec"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CBOW (sg=0 i.e., Default)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = [gensim.utils.simple_preprocess(document) for document in list(df.description)]\n",
    "\n",
    "model_cbow = gensim.models.Word2Vec(documents, vector_size=300, window=10, min_count=2, workers=10)\n",
    "\n",
    "model_cbow.train(documents,total_examples=len(documents),epochs=150)\n",
    "\n",
    "customized_word2vec_news = df['title'].apply(lambda sentence: sentence_vector(sentence, model_cbow.wv))\n",
    "\n",
    "dataframe_customized_news = list(customized_word2vec_news)\n",
    "cosine_similarity_customized = cosine_similarity(dataframe_customized_news)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "news_id = 8777\n",
    "\n",
    "print('actual news ----', '\\n', df.iloc[news_id].title)\n",
    "the_sort = sorted(list(enumerate(cosine_similarity_customized[news_id])), reverse=True, key=lambda x:x[1])[0:10]\n",
    "\n",
    "print('similar news -----')\n",
    "# close_ques = []\n",
    "for each in the_sort[1:4]:\n",
    "    # close_ques.append(each[0])\n",
    "    print(df.iloc[each[0]].title)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SKIPGRAM  (sg=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = [gensim.utils.simple_preprocess(document) for document in list(df.description)]\n",
    "\n",
    "model_skipgram = gensim.models.Word2Vec(documents, vector_size=300, window=10, min_count=2, workers=10, sg=1)\n",
    "model_skipgram.train(documents,total_examples=len(documents),epochs=150)\n",
    "\n",
    "model_skipgram.wv.save_word2vec_format('customized_word2vec.txt', binary=False)\n",
    "\n",
    "customized_word2vec_news_Skip = df['title'].apply(lambda sentence: sentence_vector(sentence, model_skipgram.wv))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "actual news ---- \n",
      " Rugby World Cup: Holly Aitchison joins Emily Scarratt in England centres for final\n",
      "similar news -----\n",
      "2023 sporting calendar: The year's main events from Women's World Cup football to Ashes series and men's rugby union World Cup\n",
      "Why were ITV hosts Holly Willoughby and Phillip Schofield at Queen's lying-in-state?\n",
      "ITV boss defends Holly Willoughby and Phillip Schofield over queue furore\n"
     ]
    }
   ],
   "source": [
    "news_id = 9882\n",
    "\n",
    "print('actual news ----', '\\n', df.iloc[news_id].title)\n",
    "the_sort = sorted(list(enumerate(cosine_similarity_customized[news_id])), reverse=True, key=lambda x:x[1])[0:10]\n",
    "\n",
    "print('similar news -----')\n",
    "# close_ques = []\n",
    "for each in the_sort[1:4]:\n",
    "    # close_ques.append(each[0])\n",
    "    print(df.iloc[each[0]].title)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LSA/SVD on Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SVD on pre trained word2vec\n",
    "svd_model = TruncatedSVD(n_components=50, algorithm='randomized', n_iter=100, random_state=122)\n",
    "lsa = svd_model.fit_transform(list(word2vec_news))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "cosine_similarity_pretrain_word2vec_lsa = cosine_similarity(lsa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "actual news ---- \n",
      " In pictures: Snow blankets parts of the UK as ld snap starts\n",
      "similar news -----\n",
      "In pictures: Snow blankets parts of the UK\n",
      "UK weather: Spring snow as parts of untry hit by ld snap\n",
      "Hot weather: Amber heat warning in place as untry braces for rerd temperatures\n"
     ]
    }
   ],
   "source": [
    "news_id = 14012\n",
    "\n",
    "print('actual news ----', '\\n', df.iloc[news_id].title)\n",
    "the_sort = sorted(list(enumerate(cosine_similarity_pretrain_word2vec_lsa[news_id])), reverse=True, key=lambda x:x[1])[0:10]\n",
    "\n",
    "print('similar news -----')\n",
    "# close_ques = []\n",
    "for each in the_sort[1:4]:\n",
    "    # close_ques.append(each[0])\n",
    "    print(df.iloc[each[0]].title)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clustering Effect"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating Function to assign each document to a cluster based on bagofwords, word2vec etc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def assign_doc_to_clus(cluster_labels, df_title):\n",
    "    outer_clas = []\n",
    "    for i in range(5):\n",
    "        cluster_documents = []\n",
    "        for j in range(len(df_title)):\n",
    "            if cluster_labels[j] == i:\n",
    "                cluster_documents.append(df_title[j])\n",
    "        outer_clas.append(cluster_documents)\n",
    "    return outer_clas"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "KMeans on default (i.e., Best) CountVectorizer and TfIdf Vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CountVectorizer With KMeans\n",
    "km_cv = KMeans(n_clusters = 5)\n",
    "km_cv.fit(lemma_vector_countvect)\n",
    "labels_cv = km_cv.labels_\n",
    "\n",
    "cv_assign = assign_doc_to_clus(labels_cv, list(df.title))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TfIdfVectorizer With KMeans\n",
    "km_tfidf = KMeans(n_clusters = 5)\n",
    "km_tfidf.fit(lemma_vector_tfidf)\n",
    "labels_tfidf = km_tfidf.labels_\n",
    "\n",
    "tfidf_assign = assign_doc_to_clus(labels_tfidf, list(df.title))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Kmeans on Word2Vec Pre-trained"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\faree\\Desktop\\google_NQ_Assignment\\.venv-3.9.0-text-project\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Word2Vec With KMeans\n",
    "km_word2vec = KMeans(n_clusters = 5)\n",
    "km_word2vec.fit(list(word2vec_news))\n",
    "labels_word2vec = km_word2vec.labels_\n",
    "\n",
    "pretrain_word2vec_assign = assign_doc_to_clus(labels_word2vec, list(df.title))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Kmeans on Glove Pre-trained"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\faree\\Desktop\\google_NQ_Assignment\\.venv-3.9.0-text-project\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Glove With KMeans\n",
    "km_glove = KMeans(n_clusters = 5)\n",
    "km_glove.fit(list(glove_news))\n",
    "labels_glove = km_glove.labels_\n",
    "\n",
    "pretrain_glove_assign = assign_doc_to_clus(labels_glove, list(df.title))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Kmeans on Customized Word2Vec - CBOW (Default) and Skipgram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CBOW With KMeans\n",
    "km_cbow = KMeans(n_clusters = 5)\n",
    "km_cbow.fit(list(customized_word2vec_news))\n",
    "labels_customized_word2vec_cbow = km_cbow.labels_\n",
    "\n",
    "customized_cbow_assign = assign_doc_to_clus(labels_customized_word2vec_cbow, list(df.title))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SKipgram With KMeans\n",
    "km_skipgram = KMeans(n_clusters = 5)\n",
    "km_skipgram.fit(list(customized_word2vec_news_Skip))\n",
    "labels_customized_word2vec_skipgram = km_skipgram.labels_\n",
    "\n",
    "customized_skipgram_assign = assign_doc_to_clus(labels_customized_word2vec_skipgram, list(df.title))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Kmeans on LSA_Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# word2vec with SVD With KMeans\n",
    "km_word2vec_lsa = KMeans(n_clusters = 5)\n",
    "km_word2vec_lsa.fit(list(lsa))\n",
    "labels_customized_word2vec_lsa = km_word2vec_lsa.labels_\n",
    "\n",
    "word2vec_lsa_assign = assign_doc_to_clus(labels_customized_word2vec_lsa, list(df.title))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Saving the models"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bag of words most accurate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform the data and fit KMeans\n",
    "vectorizer = TfidfVectorizer(ngram_range=(1,2), stop_words='english')\n",
    "\n",
    "vectorizer.fit(df['headline_lemmatization'])\n",
    "\n",
    "# Save the vectorizer and KMeans to files\n",
    "\n",
    "with open('modelstest/bag_of_words_best/vectorizer.pkl', 'wb') as f:\n",
    "    pickle.dump(vectorizer, f)\n",
    "    \n",
    "with open('modelstest/bag_of_words_best/kmeans.pkl', 'wb') as f:\n",
    "    pickle.dump(km_tfidf, f)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Word2vec saving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\faree\\Desktop\\google_NQ_Assignment\\.venv-3.9.0-text-project\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Save the vectorized data to files\n",
    "with open('modelstest/word2vec_best/pretrain_word2vec_assign.pkl', 'wb') as f:\n",
    "    pickle.dump(pretrain_word2vec_assign, f)\n",
    "\n",
    "# Save the KMeans to files\n",
    "with open('modelstest/word2vec_best/kmeans_word2vec.pkl', 'wb') as f:\n",
    "    pickle.dump(km_word2vec, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "glove saving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\faree\\Desktop\\google_NQ_Assignment\\.venv-3.9.0-text-project\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Save the vectorized data to files\n",
    "with open('modelstest/glove_best/pretrain_glove_assign.pkl', 'wb') as f:\n",
    "    pickle.dump(pretrain_glove_assign, f)\n",
    "\n",
    "# Save the KMeans to files\n",
    "with open('modelstest/glove_best/kmeans_glove.pkl', 'wb') as f:\n",
    "    pickle.dump(km_glove, f)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Customized word2vec saving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the vectorized data to files\n",
    "with open('modelstest/customized_word2vec_best/customized_skipgram_assign.pkl', 'wb') as f:\n",
    "    pickle.dump(customized_skipgram_assign, f)\n",
    "\n",
    "# Save the KMeans to files\n",
    "with open('modelstest/customized_word2vec_best/kmeans_customized_skipgram.pkl', 'wb') as f:\n",
    "    pickle.dump(km_skipgram, f)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LSA/SVD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the KMeans to files\n",
    "with open('modelstest/svd_best/svd_lsa_d.pkl', 'wb') as f:\n",
    "    pickle.dump(lsa, f)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### X___________________________________________________________________________________X"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv-3.9.0-text-project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
